{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "y_predicted      1    All\n",
      "y                        \n",
      "0.0            100    100\n",
      "1.0          10000  10000\n",
      "All          10100  10100\n",
      "f1 score is 0.9950248756218906\n",
      "AUC Score is 0.488306\n",
      "Accuracy score  is 99.00990099009901\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute(y_act,y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    True Positive - actual = 1, predicted = 1\n",
    "    False Positive - actual =1, predicted = 0\n",
    "    False Negative - actual =0,predicted = 1\n",
    "    True Negative  - actual = 0, predicted = 0\n",
    "    \"\"\"\n",
    "    tp = sum((y_act == 1) & (y_pred == 1)) #Calculate True Positive\n",
    "    tn = sum((y_act == 0) & (y_pred == 0)) #Calculate True Negative\n",
    "    fn = sum((y_act == 1) & (y_pred == 0)) #Calculate False Negative\n",
    "    fp = sum((y_act == 0) & (y_pred == 1)) #Calculate False Positive\n",
    "    return tp,tn,fn,fp     \n",
    "\n",
    "\n",
    "def compute_accuracy(tp,tn,fn,fp):\n",
    "    \"\"\"\n",
    "    Accuracy\n",
    "    \"\"\"\n",
    "    return ((tp + tn) * 100) / float(tp+tn+fn+fp) #Calculate Accuracy\n",
    "\n",
    "def compute_precision(tp,fp):\n",
    "    \"\"\"\n",
    "    Precision \n",
    "    \"\"\"\n",
    "    return (tp * 100)/ float(tp+fp)  #Calculate Precision\n",
    "\n",
    "def compute_recall(tp,fn):\n",
    "    \"\"\"\n",
    "    Recall \n",
    "    \"\"\"\n",
    "    return (tp * 100)/ float(tp+fn)  #Calculate Recall\n",
    "\n",
    "def tptn(y_act,y_prob):\n",
    "    \"\"\"\n",
    "    This function returns TPR and FPR\n",
    "    \"\"\"\n",
    "    threshold = np.random.uniform(0,1,size =10000)  #Randomly Selecting values between [0,1]\n",
    "    reverse_order = np.sort(threshold)[::-1]  #Sorted values by descending order\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for i in reverse_order :                   #Iterating the loop\n",
    "        df[i]= ( y_prob >= i ).astype('int')   #Based on threshold value to classify\n",
    "        tp,tn,fn,fp = compute(y_act,df[i])      #Call the compute function\n",
    "        tpr.append((compute_recall(tp,fn))/100)  #Append all True Positive rates to list\n",
    "        fpr.append(compute_falsepositive(fp,tn))  #Append all False Positive rates to list \n",
    "    return tpr,fpr\n",
    "\n",
    "def compute_falsepositive(fp,tn):\n",
    "    \"\"\"\n",
    "    False Positive Rate \n",
    "    \"\"\"\n",
    "    return (fp)/ float(tn+fp)  #Calculate False Positie rate\n",
    "\n",
    "def compute_f1_score(y_act,y_pred):\n",
    "    \"\"\"\n",
    "    Calcualte F1 Score\n",
    "    \"\"\"\n",
    "    tp,tn,fn,fp = compute(y_act,y_pred)         #Call the compute function\n",
    "    precision = compute_precision(tp,fp)/100    #Call the precision function\n",
    "    recall = compute_recall(tp,fn)/100         #Call the recall function\n",
    "    f1_score = (2*precision*recall)/(recall+precision) #Calculate f1 score \n",
    "    return f1_score\n",
    "    \n",
    "df = pd.read_csv(\"5_a.csv\")                    #Read the CSV file stored into df\n",
    "df['y_predicted']= (df['proba'] >= 0.5).astype('int')   #Based on threshold value to classify\n",
    "df_confusion = pd.crosstab(df['y'], df['y_predicted'],margins=True)  #computing confusion matrix\n",
    "print(\"Confusion Matrix :\")\n",
    "print(df_confusion)\n",
    "y_prob = df['proba']\n",
    "y_act = df['y']\n",
    "y_pred = df['y_predicted']\n",
    "tp,tn,fn,fp = compute(y_act,y_pred)\n",
    "Accuracy = compute_accuracy(tp,tn,fn,fp)  #computing accuracy\n",
    "precision = compute_precision(tp,fp)  #computing precision\n",
    "recall = compute_recall(tp,fn)        #Computing recall\n",
    "f1_score = compute_f1_score(y_act,y_pred)  #Calling F1 Score \n",
    "print(\"f1 score is \" + str(f1_score))\n",
    "tparray,fparray = tptn(y_act,y_prob)   #calculating tpr,fpr \n",
    "a= np.asarray(tparray, dtype = float)\n",
    "b= np.asarray(fparray, dtype = float)\n",
    "AUC = np.trapz(a, b)            #Calculating Area under Curve\n",
    "print(\"AUC Score is \" + str(AUC))\n",
    "print(\"Accuracy score  is \" + str(Accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "y_predicted     0    1    All\n",
      "y                            \n",
      "0.0          9761  239  10000\n",
      "1.0            45   55    100\n",
      "All          9806  294  10100\n",
      "f1 score is 0.27918781725888325\n",
      "AUC Score is 0.9377635\n",
      "Accuracy score  is 97.18811881188118\n"
     ]
    }
   ],
   "source": [
    "# write your code# write your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute(y_act,y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    True Positive - actual = 1, predicted = 1\n",
    "    False Positive - actual =1, predicted = 0\n",
    "    False Negative - actual =0,predicted = 1\n",
    "    True Negative  - actual = 0, predicted = 0\n",
    "    \"\"\"\n",
    "    tp = sum((y_act == 1) & (y_pred == 1)) #Calculate True Positive\n",
    "    tn = sum((y_act == 0) & (y_pred == 0)) #Calculate True Negative\n",
    "    fn = sum((y_act == 1) & (y_pred == 0)) #Calculate False Negative\n",
    "    fp = sum((y_act == 0) & (y_pred == 1)) #Calculate False Positive\n",
    "    return tp,tn,fn,fp    \n",
    "\n",
    "def compute_accuracy(tp,tn,fn,fp):\n",
    "    \"\"\"\n",
    "    Accuracy\n",
    "    \"\"\"\n",
    "    return ((tp + tn) * 100) / float(tp+tn+fn+fp)  #Calculate Accuracy\n",
    "\n",
    "def compute_precision(tp,fp):\n",
    "    \"\"\"\n",
    "    Precision \n",
    "    \"\"\"\n",
    "    return (tp * 100)/ float(tp+fp) #Calculate precision\n",
    "\n",
    "def compute_falsepositive(fp,tn):\n",
    "    \"\"\"\n",
    "    False Positive Rate \n",
    "    \"\"\"\n",
    "    return (fp)/ float(tn+fp) #Calculate False positive\n",
    "\n",
    "def compute_recall(tp,fn):\n",
    "    \"\"\"\n",
    "    Recall \n",
    "    \"\"\"\n",
    "    return (tp * 100)/ float(tp+fn)  #Calculate Recall\n",
    "\n",
    "def compute_f1_score(y_act,y_pred):\n",
    "    \"\"\"\n",
    "    Calcualte F1 Score\n",
    "    \"\"\"\n",
    "    tp,tn,fn,fp = compute(y_act,y_pred)         #Call the compute function\n",
    "    precision = compute_precision(tp,fp)/100    #Call the precision function\n",
    "    recall = compute_recall(tp,fn)/100         #Call the recall function\n",
    "    f1_score = (2*precision*recall)/(recall+precision) #Calculate f1 score \n",
    "    return f1_score\n",
    "\n",
    "def tptn(y_act,y_prob):\n",
    "    \"\"\"\n",
    "    This function returns TPR and FPR\n",
    "    \"\"\"\n",
    "    threshold = np.random.uniform(0,1,size =10000)  #Randomly Selecting values between [0,1]\n",
    "    reverse_order = np.sort(threshold)[::-1]  #Sorted values by descending order\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for i in reverse_order :                   #Iterating the loop\n",
    "        df[i]= ( y_prob >= i ).astype('int')   #Based on threshold value to classify\n",
    "        tp,tn,fn,fp = compute(y_act,df[i])      #Call the compute function\n",
    "        tpr.append((compute_recall(tp,fn))/100)  #Append all True Positive rates to list\n",
    "        fpr.append(compute_falsepositive(fp,tn))  #Append all False Positive rates to list \n",
    "    return tpr,fpr\n",
    "\n",
    "    \n",
    "df = pd.read_csv(\"5_b.csv\")   #Read the CSV file stored into df\n",
    "df['y_predicted']= (df['proba'] >= 0.5).astype('int')  #Based on threshold value to classify\n",
    "df_confusion = pd.crosstab(df['y'], df['y_predicted'],margins=True) #computing confusion matrix\n",
    "print(\"Confusion Matrix :\")\n",
    "print(df_confusion)\n",
    "y_prob = df['proba']\n",
    "y_act = df['y']\n",
    "y_pred = df['y_predicted']\n",
    "tp,tn,fn,fp = compute(y_act,y_pred)    #Computing tp,tn,fp,fn\n",
    "Accuracy = compute_accuracy(tp,tn,fn,fp)   #Computing Accuracy\n",
    "precision = compute_precision(tp,fp)  #Computing precision\n",
    "recall = compute_recall(tp,fn)   #Compute recall  \n",
    "f1_score = compute_f1_score(y_act,y_pred)  #Compute f1 score \n",
    "print(\"f1 score is \" + str(f1_score))\n",
    "tparray,fparray = tptn(y_act,y_prob)  #Calculate tpr,fpr \n",
    "a= np.asarray(tparray, dtype = float)\n",
    "b= np.asarray(fparray, dtype = float)\n",
    "AUC = np.trapz(a, b)     #Calculate Area under Curve\n",
    "print(\"AUC Score is \" + str(AUC))\n",
    "print(\"Accuracy score  is \" + str(Accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false positives} + 100 \\times \\text{numebr of false negatives}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points < number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6432476012102228\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "def compute(y_act,y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    True Positive - actual = 1, predicted = 1\n",
    "    False Positive - actual =1, predicted = 0\n",
    "    False Negative - actual =0,predicted = 1\n",
    "    True Negative  - actual = 0, predicted = 0\n",
    "    \"\"\"\n",
    "    tp = sum((y_act == 1) & (y_pred == 1)) #Calculate True Positive\n",
    "    tn = sum((y_act == 0) & (y_pred == 0)) #Calculate True Negative\n",
    "    fn = sum((y_act == 1) & (y_pred == 0)) #Calculate False Negative\n",
    "    fp = sum((y_act == 0) & (y_pred == 1)) #Calculate False Positive\n",
    "    return tp,tn,fn,fp    \n",
    "\n",
    "df = pd.read_csv(\"5_c.csv\")   #Read the CSV file stored into df\n",
    "y_prob = df['prob']\n",
    "y_act = df['y']\n",
    "threshold = np.random.uniform(0,1,size =10000) #Randomly Selecting threshold values \n",
    "setin = {}\n",
    "for i in threshold:\n",
    "    df[i]= ( y_prob >= i ).astype('int')   #Based on threshold to predict\n",
    "    tp,tn,fn,fp = compute(y_act,df[i])   #Compute tp,tn,fn,fp\n",
    "    A = 500*fp + 100 * fn     # Calcualting function A = 500 ×number of false positives+100×numebr of false negatives\n",
    "    setin[i] = A\n",
    "sorted_x = sorted(setin.items(), key=lambda kv: kv[1])  #Sorting ascending order based on Value A\n",
    "print(sorted_x[0][0])  #Printing best threshod value with corresponding lowest values of metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error is 177.16569974554707\n",
      "MAPE is 0.1291202994009687\n",
      "R2 Square Error is 0.9563582786990937\n"
     ]
    }
   ],
   "source": [
    "def mse(y, y_pred):\n",
    "    \"\"\"\n",
    "    This function returns Mean Square Error \n",
    "    \"\"\"\n",
    "    return np.square(y - y_pred).mean()  # Mean of the Square of the difference of Predicted and Actual value \n",
    "    \n",
    "def mape(y, y_pred):\n",
    "    \"\"\"\n",
    "    This  function return mape \n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(y-y_pred))/np.sum(y)  #Sum of absolute values of predicted and actual value/sum of actual values \n",
    "\n",
    "def rse(y, y_pred):\n",
    "    \"\"\"\n",
    "    This function return r square value\n",
    "    \"\"\"\n",
    "    ymean = np.mean(y)                       #Calculating the mean of actual value\n",
    "    ss_tot = np.sum(np.square(y-ymean))      #Calculating ss_total\n",
    "    ss_res = np.sum(np.square(y-y_pred))  #  Refer this document for calcualation  https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions\n",
    "    return 1-(ss_res/ss_tot)              #Calculating Coefficent of Determination\n",
    "    \n",
    "df = pd.read_csv(\"5_d.csv\")\n",
    "mse = mse(df['y'],df['pred'])     #Calling mean square error function\n",
    "mmape = mape(df['y'],df['pred'])   #Calling mape function\n",
    "r2 = rse(df['y'],df['pred'])      #Calling R square function\n",
    "print(\"Mean Square Error is \" + str(mse))\n",
    "print(\"MAPE is \" + str(mmape))\n",
    "print(\"R2 Square Error is \"  + str(r2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation :\n",
    "1. For imbalanced datasets,accuracy does not matter only it depends on AUC Score to find better classification techhnique\n",
    "2. As observed in 5a_csv, no of positive points is much greater than negative points,AUC Score getting 0.488306\n",
    "3. As observed in 5b_csv, no of negative points is much greater than positive points,AUC Score getting 0.9377635\n",
    "4. Compared to 2 & 3, 5b_csv(3) is good for classification \n",
    "5. A Perfect classifier has 100% true positive rate and 0% false positive rate (0 false positive).\n",
    "6. R2 Square error must be in between 0 to 1 .For mean Square Error there is no limit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
